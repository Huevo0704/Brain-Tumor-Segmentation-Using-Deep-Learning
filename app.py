# app.py

import gradio as gr
import tensorflow as tf
import numpy as np
import cv2
import os
from skimage.filters import gaussian
from skimage.segmentation import active_contour

# Import c√°c h√†m ƒë√£ ƒë∆∞·ª£c t·ªï ch·ª©c l·∫°i t·ª´ utils.py
from utils import (
    combined_loss, dice_coefficient, iou, sensitivity, specificity,
    segment_tumor_adaptive, calculate_final_tumor_area, visualize_decomposition
)

# --- C·∫§U H√åNH V√Ä T·∫¢I MODEL ---
MODEL_PATH = "U_Net_model_attention_512.keras" # File model ph·∫£i n·∫±m c√πng th∆∞ m·ª•c
IMG_SIZE = 512
KNOWN_IMAGE_DPI = 96
INCH_TO_MM = 25.4

# H√†m t·∫£i model (ch·∫°y 1 l·∫ßn)
@gr.cache
def load_trained_model(path):
    print("----- ƒêang t·∫£i m√¥ h√¨nh... -----")
    if not os.path.exists(path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file model t·∫°i '{path}'.")
    custom_objects = {
        'combined_loss': combined_loss,
        'dice_coefficient': dice_coefficient, 'iou': iou,
        'sensitivity': sensitivity, 'specificity': specificity
    }
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    model = tf.keras.models.load_model(path, custom_objects=custom_objects)
    print("----- T·∫£i m√¥ h√¨nh th√†nh c√¥ng! -----")
    return model

model = load_trained_model(MODEL_PATH)

# --- H√ÄM LOGIC CH√çNH ---
def full_analysis_pipeline(
    input_image, dpi,
    median_kernel,
    adaptive_block_size, adaptive_c,
    ac_alpha, ac_beta, ac_gamma
):
    if input_image is None:
        return None, None, None, None, "Vui l√≤ng t·∫£i ·∫£nh l√™n."

    # --- 1. Ti·ªÅn x·ª≠ l√Ω v√† D·ª± ƒëo√°n U-Net ---
    original_shape = input_image.shape[:2]
    image_gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY) if len(input_image.shape) > 2 else input_image
    image_resized = cv2.resize(image_gray, (IMG_SIZE, IMG_SIZE))
    input_tensor = np.expand_dims(image_resized / 255.0, axis=[0, -1]).astype(np.float32)
    
    pred_mask_unet = model.predict(input_tensor)[0]
    pred_mask_unet_binary = (pred_mask_unet > 0.5).astype(np.uint8).squeeze()
    
    if np.count_nonzero(pred_mask_unet_binary) == 0:
        return input_image, None, None, None, "M√¥ h√¨nh U-Net kh√¥ng ph√°t hi·ªán th·∫•y kh·ªëi u."

    # --- 2. H·∫≠u x·ª≠ l√Ω Giai ƒëo·∫°n 1: Median Blur ---
    if median_kernel % 2 == 0: median_kernel += 1 # Kernel ph·∫£i l√† s·ªë l·∫ª
    mask_median = (cv2.medianBlur(pred_mask_unet_binary * 255, median_kernel) > 127).astype(np.uint8)

    # --- 3. H·∫≠u x·ª≠ l√Ω Giai ƒëo·∫°n 2: Adaptive Thresholding ---
    mask_adaptive = segment_tumor_adaptive(mask_median * 255, adaptive_block_size, adaptive_c)

    # --- 4. H·∫≠u x·ª≠ l√Ω Giai ƒëo·∫°n 3: Active Contour (Snake) ---
    final_mask = mask_adaptive # B·∫Øt ƒë·∫ßu v·ªõi mask t·ª´ b∆∞·ªõc tr∆∞·ªõc
    smoothed_image = gaussian(image_resized, sigma=1)
    contours, _ = cv2.findContours(mask_adaptive * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        try:
            initial_snake = np.squeeze(max(contours, key=cv2.contourArea))
            if len(initial_snake) > 2: # C·∫ßn √≠t nh·∫•t 3 ƒëi·ªÉm ƒë·ªÉ t·∫°o contour
                snake = active_contour(smoothed_image, initial_snake, alpha=ac_alpha, beta=ac_beta, gamma=ac_gamma)
                temp_mask = np.zeros_like(final_mask)
                cv2.fillPoly(temp_mask, [snake.astype(np.int32)], 255)
                final_mask = (temp_mask / 255).astype(np.uint8)
        except Exception as e:
            print(f"L·ªói Active Contour: {e}. S·ª≠ d·ª•ng mask t·ª´ b∆∞·ªõc tr∆∞·ªõc.")
            # Gi·ªØ nguy√™n final_mask = mask_adaptive

    # --- 5. T√≠nh to√°n di·ªán t√≠ch ---
    pixel_area_mm2 = (INCH_TO_MM / dpi) ** 2
    geom_area, pixel_area = calculate_final_tumor_area(final_mask, pixel_area_mm2)
    
    # --- 6. T·∫°o ·∫£nh k·∫øt qu·∫£ v√† tr·ª±c quan h√≥a ---
    # Resize mask cu·ªëi c√πng v·ªÅ k√≠ch th∆∞·ªõc ·∫£nh g·ªëc
    final_mask_resized = cv2.resize(final_mask, (original_shape[1], original_shape[0]), interpolation=cv2.INTER_NEAREST)
    
    # T·∫°o ·∫£nh t√¥ m√†u
    output_image_color = cv2.cvtColor(input_image, cv2.COLOR_GRAY2BGR) if len(input_image.shape) < 3 else input_image.copy()
    red_overlay = np.zeros_like(output_image_color)
    red_overlay[final_mask_resized == 1] = [0, 0, 255] # BGR
    final_colored_image = cv2.addWeighted(output_image_color, 1.0, red_overlay, 0.6, 0)

    # T·∫°o ·∫£nh tam gi√°c h√≥a
    triangulated_image = visualize_decomposition(cv2.resize((image_resized * 255).astype(np.uint8), (512, 512)), final_mask)
    
    # T·∫°o chu·ªói k·∫øt qu·∫£
    result_text = (
        f"--- K·∫æT QU·∫¢ PH√ÇN T√çCH ---\n"
        f"Di·ªán t√≠ch (ƒê·∫øm Pixel): {pixel_area:.2f} mm¬≤\n"
        f"Di·ªán t√≠ch (H√¨nh h·ªçc): {geom_area:.2f} mm¬≤"
    )

    return final_colored_image, final_mask_resized * 255, triangulated_image, result_text


# --- T·∫†O GIAO DI·ªÜN GRADIO ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üî¨ Demo Ph√¢n t√≠ch v√† Ph√¢n ƒëo·∫°n Kh·ªëi u N√£o To√†n di·ªán")
    gr.Markdown("T·∫£i l√™n ·∫£nh MRI, ƒëi·ªÅu ch·ªânh c√°c tham s·ªë h·∫≠u x·ª≠ l√Ω v√† xem k·∫øt qu·∫£ ph√¢n t√≠ch chi ti·∫øt.")
    
    with gr.Row():
        with gr.Column(scale=1):
            # C·ªòT ƒê·∫¶U V√ÄO
            input_image = gr.Image(type="numpy", label="T·∫£i ·∫£nh MRI t·∫°i ƒë√¢y")
            dpi_input = gr.Slider(minimum=50, maximum=600, value=96, step=1, label="DPI c·ªßa ·∫£nh (ƒë·ªÉ t√≠nh mm¬≤)")
            
            with gr.Accordion("Tinh ch·ªânh c√°c b∆∞·ªõc h·∫≠u x·ª≠ l√Ω", open=False):
                gr.Markdown("### Giai ƒëo·∫°n 1: Median Blur (L√†m m·ªãn mask)")
                median_kernel = gr.Slider(minimum=3, maximum=15, value=5, step=2, label="K√≠ch th∆∞·ªõc Kernel")
                
                gr.Markdown("### Giai ƒëo·∫°n 2: Adaptive Thresholding (Tinh ch·ªânh bi√™n)")
                adaptive_block_size = gr.Slider(minimum=3, maximum=51, value=21, step=2, label="Block Size")
                adaptive_c = gr.Slider(minimum=1, maximum=20, value=5, label="Gi√° tr·ªã C")
                
                gr.Markdown("### Giai ƒëo·∫°n 3: Active Contour (L√†m m∆∞·ª£t bi√™n)")
                ac_alpha = gr.Slider(minimum=0.001, maximum=0.1, value=0.01, step=0.001, label="Alpha (ƒê·ªô co d√£n)")
                ac_beta = gr.Slider(minimum=0.01, maximum=1.0, value=0.1, step=0.01, label="Beta (ƒê·ªô c·ª©ng)")
                ac_gamma = gr.Slider(minimum=0.001, maximum=0.1, value=0.01, step=0.001, label="Gamma (L·ª±c b√™n ngo√†i)")
            
            analyze_button = gr.Button("B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch", variant="primary")

        with gr.Column(scale=2):
            # C·ªòT ƒê·∫¶U RA
            gr.Markdown("## K·∫øt qu·∫£ Ph√¢n t√≠ch")
            output_colored = gr.Image(label="K·∫øt qu·∫£ Ph√¢n ƒëo·∫°n (ƒê√£ t√¥ m√†u)")
            with gr.Row():
                output_mask = gr.Image(label="Mask nh·ªã ph√¢n cu·ªëi c√πng")
                output_triangulated = gr.Image(label="Tr·ª±c quan h√≥a t√≠nh di·ªán t√≠ch")
            
            output_textbox = gr.Textbox(label="Th√¥ng tin Di·ªán t√≠ch")

    # K·∫øt n·ªëi logic
    analyze_button.click(
        fn=full_analysis_pipeline,
        inputs=[
            input_image, dpi_input,
            median_kernel,
            adaptive_block_size, adaptive_c,
            ac_alpha, ac_beta, ac_gamma
        ],
        outputs=[output_colored, output_mask, output_triangulated, output_textbox]
    )
    
    gr.Examples(
        examples=[["demo_images/demo_image_1.jpg", 96, 5, 21, 5, 0.01, 0.1, 0.01]],
        inputs=[
            input_image, dpi_input,
            median_kernel,
            adaptive_block_size, adaptive_c,
            ac_alpha, ac_beta, ac_gamma
        ]
    )

if __name__ == "__main__":
    demo.launch(debug=True)
